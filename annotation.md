The url of the annotated images: https://drive.google.com/drive/folders/1bbeWTDKo3_nnrYswJBsUybdVZLlxlI6S?usp=sharing

DEXTR Summary:

Deep Extreme Cut (DEXTR) is a technique that obtains an object segmentation from its four extreme points: the left-most, right-most, top, and bottom pixels. DEXTR can also incorporate more points beyond the extreme ones, which further refines the quality. 
One of the most common ways to perform weakly supervised segmentation is drawing a bounding box around the object of interest. However, in order to draw the corners of a bounding box, the user has to click points outside the object, drag the box diagonally, and adjust it several times to obtain a tight, accurate bounding box. This process is cognitively demanding, with increased error rates and labelling times.
Papadopoulos have shown a much more efficient way of obtaining a bounding box using extreme clicks, spending on average 7.2 seconds instead of 34.5 seconds required for drawing a bounding box around an object.
Extreme-clicking annotations provide more information than a bounding box; they contain four points that are on the boundary of the object, from which one can easily obtain the bounding-box. We use extreme points for object segmentation leveraging their two main outcomes: the points and their inferred bounding box.
Image and video segmentation generate dense predictions where each pixel receives a (potentially different) output classification. Deep learning algorithms, especially Convolutional Neural Networks (CNNs), were adapted to this scenario by removing the final fully connected layers to produce dense predictions.
The authors choose ResNet-101 as the backbone of our architecture, as it has been proven successful in a variety of segmentation methods. They remove the fully connected layers as well as the max pooling layers in the last two stages to preserve acceptable output resolution for dense prediction, and they introduce atrous convolutions in the last two stages to maintain the same receptive field. After the last ResNet-101 stage, they introduce a pyramid scene parsing module to aggregate global context to the final feature map.
The output of the CNN is a probability map representing whether a pixel belongs to the object that we want to segment or not.
The common annotation pipeline for segmentation can also be assisted by DEXTR. In this framework, instead of detailed polygon labels, the workload of the annotator is reduced to only providing the extreme points of an object, and DEXTR produces the desired segmentation. In this pipeline, the labelling cost is reduced by a factor of 10 (from 79 seconds needed for a mask, to 7.2 seconds needed for the extreme clicks)
Where the user labels the extreme points of an object but is nevertheless not satisfied with the obtained results can annotate an extra point (not extreme) in the region that segmentation fails and expects for a refined result. Given the nature of extreme points, they expect that the extra point also lies in the boundary of the object.
They first train DEXTR on a first split of a training set of images, using the 4 extreme points as input. For the extra point, they infer on an image of the second split of the training set and compute the accuracy of its segmentation. If the segmentation is accurate (e.g., IoU ? 0.8), the image is excluded from further processing. In the opposite case (IoU < 0.8), they select a fifth point in the erroneous area. To simulate human behavior, they perturbate its location and we train the network with 5 points as input.
The results trained on DEXTR’s masks are significantly better than those trained from the ground truth on the same budget (e.g., 70% IoU at 7-minute annotation time vs. 46% with the same budget, or 1h10 instead of 7 minutes to reach the same 70% accuracy). DEXTR’s annotations reach practically the same performance than ground truth when given the same number of annotated images.
